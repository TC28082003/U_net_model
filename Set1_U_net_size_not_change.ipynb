{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bibliothèques standards de Python ---\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "from math import sqrt\n",
    "\n",
    "# --- Bibliothèques scientifiques et de traitement d'images ---\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from scipy.signal.windows import hann\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "# --- Bibliothèques de Machine Learning et Deep Learning ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, ReLU, LeakyReLU, add, subtract\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "# --- Métrique d'évaluation avancée ---\n",
    "import lpips\n",
    "import torch\n",
    "\n",
    "# --- Outils d'analyse ---\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b93a76",
   "metadata": {},
   "source": [
    "Paths: make sure that all yours images in clean_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740415a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dir = \"images/clean/\"\n",
    "test_dir = \"ready for label\"\n",
    "output_dir = \"images/denoised/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb6077",
   "metadata": {},
   "source": [
    "Loading images:\n",
    "\n",
    "+ Data Base:\n",
    "    - i used more than 500 ultrasound images saved in clean files as training set for my model.\n",
    "    - 47 images for model testing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04802715",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 42 # You can change this value for different splits\n",
    "print(\"Scanning the clean image database...\")\n",
    "all_clean_paths = sorted(glob.glob(os.path.join(clean_dir, \"**/*.jpg\"), recursive=True))\n",
    "# all_clean_paths += sorted(glob.glob(os.path.join(clean_dir, \"**/*.png\"), recursive=True))\n",
    "\n",
    "#test_paths = sorted(glob.glob(os.path.join(test_dir, \"**/*.jpg\"), recursive=True))\n",
    "test_paths = sorted(glob.glob(os.path.join(test_dir, \"**/*.png\"), recursive=True))\n",
    "if not all_clean_paths:\n",
    "    raise FileNotFoundError(f\"No clean images found in '{clean_dir}'.\")\n",
    "\n",
    "print(f\"{len(all_clean_paths)} clean images found.\")\n",
    "\n",
    "#--- Split the dataset into training, validation, and testing sets ---\n",
    "train_paths, val_paths = train_test_split(all_clean_paths, test_size=0.2, random_state=SEED) \n",
    "\n",
    "print(f\"\\nDataset split into:\")\n",
    "print(f\" - {len(train_paths)} images for training\")\n",
    "print(f\" - {len(val_paths)} images for validation\")\n",
    "print(f\" - {len(test_paths)} images for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73debc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_random_samples(image_paths, num_images=5, title=\"Sample Images\"):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    \n",
    "    indices = np.random.choice(len(image_paths), num_images, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img_path = image_paths[idx]\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            print(f\"Không thể đọc ảnh: {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Plot image on grid\n",
    "        plt.subplot(1, num_images, i + 1)\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(f\"Img Index: {idx}\\n{img_rgb.shape}\") # Display image size for verification\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Call the function to display ---\n",
    "print(\"Displaying sample clean images from the Train set:\")\n",
    "visualize_random_samples(train_paths, num_images=5, title=\"Sample clean images from the Train set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba11f558",
   "metadata": {},
   "source": [
    "Data Pipeline\n",
    "\n",
    "GPU Memory (VRAM): When training, Deep Learning stores not only the image, but also millions of parameters (weights) and dozens of intermediate transformation layers (feature maps).\n",
    "\n",
    "A 720x960 image when fed into a CNN network can swell to several GB in VRAM. If not chopped, the GPU will immediately report an Out Of Memory (OOM) error.\n",
    "\n",
    "Local feature learning: As explained, chopping helps the network focus on learning the detailed structure (texture) instead of trying to learn the overall layout.\n",
    "\n",
    "Another problem is that when you train a deeplearning model, your inputs have to be of the same shape, and if you don't split them you need to reshape them. So it lost a lot of detail and it was also harder to return them to their original shape.\n",
    "\n",
    "Clean Image -> Add Noise -> Noisy Image\n",
    "\n",
    "Used a dataset of clean images and simulated real-world conditions by adding Gaussian Noise\n",
    "\n",
    "I sliced images into small 256x256 patches. You can change to 512 for larger patches and 128 for smaller patches (it depends on your GPU memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Pipeline Parameters ---\n",
    "PATCH_SIZE = 256 # Change to 512 for larger patches and 128 for smaller patches (it depends on your GPU memory)   \n",
    "BATCH_SIZE = 32    \n",
    "NOISE_STD_DEV = 30/255.0  #  (Try to test with sigma = 10,20,30) different levels of noise for set1\n",
    "BUFFER_SIZE = 1000 \n",
    "\n",
    "def load_and_crop_image(path):\n",
    "\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.io.decode_image(image, channels=3, expand_animations=False)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.random_crop(image, size=[PATCH_SIZE, PATCH_SIZE, 3])\n",
    "    \n",
    "    return image\n",
    "\n",
    "def add_speckle_noise(clean_image):\n",
    "    \"\"\"\n",
    "    Speckle Noise - A characteristic of ultrasound/radar images.\n",
    "    Formula: Noisy = Image * (1 + Gaussian_Noise)\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal(shape=tf.shape(clean_image), mean=0.0, stddev=NOISE_STD_DEV, dtype=tf.float32)\n",
    "    \n",
    "    noisy_image = clean_image * (1.0 + noise)\n",
    "    \n",
    "    noisy_image = tf.clip_by_value(noisy_image, 0.0, 1.0)\n",
    "    \n",
    "    return noisy_image, clean_image\n",
    "\n",
    "def prepare_dataset(file_paths, is_training=True):\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "    ds = ds.map(load_and_crop_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if is_training:\n",
    "        ds = ds.shuffle(buffer_size=BUFFER_SIZE)\n",
    "    ds = ds.map(add_speckle_noise, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "# ---  Dataset ---\n",
    "print(\"Initializing Data Pipeline...\")\n",
    "train_ds = prepare_dataset(train_paths, is_training=True)\n",
    "val_ds = prepare_dataset(val_paths, is_training=False)\n",
    "\n",
    "print(\" Number of training batches:\", len(train_ds))\n",
    "print(\" Number of validation batches:\", len(val_ds))\n",
    "sample_noisy, sample_clean = next(iter(train_ds))\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(f\"Input (Noisy)\\nShape: {sample_noisy[0].shape}\")\n",
    "plt.imshow(sample_noisy[0]) # Display the first image in the batch\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(f\"Target (Clean)\\nShape: {sample_clean[0].shape}\")\n",
    "plt.imshow(sample_clean[0])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = test_paths[0] \n",
    "full_clean_img1 = cv2.imread(sample_path)\n",
    "full_clean_img1 = cv2.cvtColor(full_clean_img1, cv2.COLOR_BGR2RGB).astype('float32') / 255.0\n",
    "\n",
    "# Add noise to the full image\n",
    "noise1 = add_speckle_noise(tf.convert_to_tensor(full_clean_img1))[0].numpy() - full_clean_img1\n",
    "full_noisy_img1 = np.clip(full_clean_img1 + noise1, 0, 1)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Input (Noisy Full Image)\")\n",
    "plt.imshow(full_noisy_img1)\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Target (Clean Full Image)\")\n",
    "plt.imshow(full_clean_img1)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f\"Full image shape: {full_clean_img1.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_patches_weighted(patches, image_shape, patch_size, stride):\n",
    "    \"\"\"\n",
    "    Reconstructs an image from overlapping patches (weighted average).\n",
    "    Supports 3-channel color images.\n",
    "    \"\"\"\n",
    "    reconstructed = np.zeros(image_shape, dtype=np.float32)\n",
    "    weight_map = np.zeros(image_shape, dtype=np.float32)\n",
    "    \n",
    "    # Hann window 2D\n",
    "    window = hann(patch_size)\n",
    "    window_2d = np.outer(window, window) \n",
    "    \n",
    "    window_3d = window_2d[..., np.newaxis] \n",
    "    \n",
    "    patch_idx = 0\n",
    "    \n",
    "    n_h = (image_shape[0] - patch_size) // stride + 1\n",
    "    n_w = (image_shape[1] - patch_size) // stride + 1\n",
    "    \n",
    "    for i in range(n_h):\n",
    "        for j in range(n_w):\n",
    "            if patch_idx < len(patches):\n",
    "                h_start = i * stride\n",
    "                w_start = j * stride\n",
    "                \n",
    "                h_end = h_start + patch_size\n",
    "                w_end = w_start + patch_size\n",
    "                \n",
    "                reconstructed[h_start:h_end, w_start:w_end, :] += patches[patch_idx] * window_3d\n",
    "                weight_map[h_start:h_end, w_start:w_end, :] += window_3d\n",
    "                \n",
    "                patch_idx += 1\n",
    "                \n",
    "    reconstructed /= np.maximum(weight_map, 1e-8)\n",
    "    \n",
    "    return np.clip(reconstructed, 0, 1)\n",
    "\n",
    "# Helper function to extract test image into patches\n",
    "def extract_patches_sliding(image, patch_size, stride):\n",
    "    patches = []\n",
    "    h, w, c = image.shape\n",
    "    n_h = (h - patch_size) // stride + 1\n",
    "    n_w = (w - patch_size) // stride + 1\n",
    "    \n",
    "    for i in range(n_h):\n",
    "        for j in range(n_w):\n",
    "            h_start = i * stride\n",
    "            w_start = j * stride\n",
    "            patch = image[h_start:h_start+patch_size, w_start:w_start+patch_size, :]\n",
    "            patches.append(patch)\n",
    "    return np.array(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d56c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(inputs, filters):\n",
    "    \"\"\"A standard convolutional block: Conv -> BN -> ReLU\"\"\"\n",
    "    x = Conv2D(filters, (3, 3), padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape=(256, 256, 3)): # Try to test wwith (512x512x3)\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    # --- ENCODER (Downsampling) ---\n",
    "    # Block 1\n",
    "    c1 = conv_block(inputs, 32) # 32 filters\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    # Block 2\n",
    "    c2 = conv_block(p1, 64)     # 64 filters\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "\n",
    "    # --- BOTTLENECK (Bottom of the U) ---\n",
    "    b = conv_block(p2, 128)     # 128 filters\n",
    "\n",
    "    # --- DECODER (Upsampling + Skip Connections) ---\n",
    "    # Block 3 (Up 1)\n",
    "    u1 = UpSampling2D((2, 2))(b)\n",
    "    u1 = concatenate([u1, c2])  \n",
    "    c5 = conv_block(u1, 64)\n",
    "\n",
    "    # Block 4 (Up 2)    \n",
    "    u2 = UpSampling2D((2, 2))(c5)\n",
    "    u2 = concatenate([u2, c1])  \n",
    "    c7 = conv_block(u2, 32)\n",
    "\n",
    "\n",
    "    # --- OUTPUT ---\n",
    "    outputs = Conv2D(3, (1, 1), activation='sigmoid')(c7) # 3 color channels, Sigmoid to [0,1]\n",
    "\n",
    "    model = models.Model(inputs, outputs, name=\"Baseline_UNet\")\n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "unet = build_unet((256, 256, 3)) # Try to test wwith (512x512x3)\n", 
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c03820",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # Changed to 1e-4, 1e-5 \n",
    "    loss='mae',  # Focus on Mean Absolute Error for denoising\n",
    "    metrics=['mse'] \n",
    ")   \n",
    "\n",
    "# --- 5. TRAIN ---\n",
    "callbacks_set1 = [\n",
    "    EarlyStopping(patience=20, restore_best_weights=True, monitor='val_loss', verbose=1),\n",
    "    ReduceLROnPlateau(patience=5, factor=0.5, verbose=1),\n",
    "    ModelCheckpoint(\"unet_set1_structure.keras\", save_best_only=True, monitor='val_loss')\n",
    "]\n",
    "\n",
    "print(\" Starting training Set 1: Structure-Aware...\")\n",
    "history_set1 = unet.fit(\n",
    "    train_ds,\n",
    "    epochs=50,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks_set1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d726f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Function to plot the Loss and Metric comparison between Train and Validation set.\n",
    "    Input: history (return result from model.fit)\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of metrics in the training period.\n",
    "    metrics = [k for k in history.history.keys() if \"val_\" not in k]\n",
    "    \n",
    "    n_metrics = len(metrics)\n",
    "    plt.figure(figsize=(6 * n_metrics, 5))\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric != \"learning_rate\": \n",
    "            train_values = history.history[metric]\n",
    "            val_values = history.history[f'val_{metric}']\n",
    "            epochs = range(1, len(train_values) + 1)\n",
    "\n",
    "            plt.subplot(1, n_metrics, i + 1)\n",
    "            \n",
    "            # Training\n",
    "            plt.plot(epochs, train_values, 'b-o', label=f'Training {metric}', markersize=4)\n",
    "            \n",
    "            # Validation\n",
    "            plt.plot(epochs, val_values, 'r-o', label=f'Validation {metric}', markersize=4)\n",
    "            \n",
    "            plt.title(f'Training vs Validation {metric.upper()}')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel(metric.upper())\n",
    "            plt.legend()\n",
    "            plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_training_history(history_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_alex = lpips.LPIPS(net='alex') \n",
    "if torch.cuda.is_available():\n",
    "    loss_fn_alex = loss_fn_alex.cuda()\n",
    "# Prepare a list containing points\n",
    "psnr_unet, ssim_unet, lpips_unet, time_unet = [], [], [], []\n",
    "print(\" Number of test images:\", len(test_paths))\n",
    "\n",
    "test_ds = prepare_dataset(test_paths, is_training=False)\n",
    "print(\" Number of test batches:\", len(test_ds))\n",
    "\n",
    "for batch_noisy, batch_clean in test_ds:\n",
    "    start_t = time.time()\n",
    "    preds = unet.predict(batch_noisy, verbose=0)\n",
    "    end_t = time.time()\n",
    "    \n",
    "    avg_time = (end_t - start_t) / batch_noisy.shape[0]\n",
    "    \n",
    "    batch_clean_np = batch_clean.numpy()\n",
    "    \n",
    "    for i in range(len(batch_clean_np)):\n",
    "        clean_img = batch_clean_np[i]\n",
    "        denoised_img = preds[i]\n",
    "        \n",
    "        # Calculate PSNR / SSIM\n",
    "        psnr_unet.append(psnr(clean_img, denoised_img, data_range=1.0))\n",
    "        ssim_unet.append(ssim(clean_img, denoised_img, data_range=1.0, channel_axis=-1))\n",
    "        \n",
    "        # Calculate LPIPS\n",
    "        t_clean = torch.from_numpy(clean_img).permute(2, 0, 1).unsqueeze(0).float() * 2 - 1\n",
    "        t_denoised = torch.from_numpy(denoised_img).permute(2, 0, 1).unsqueeze(0).float() * 2 - 1\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            t_clean = t_clean.cuda()\n",
    "            t_denoised = t_denoised.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            lpips_unet.append(loss_fn_alex(t_clean, t_denoised).item())\n",
    "            \n",
    "        time_unet.append(avg_time)\n",
    "\n",
    "results = {} \n",
    "# Update results in the common table\n",
    "results[\"UNet\"] = {\n",
    "    \"PSNR\": np.mean(psnr_unet),\n",
    "    \"SSIM\": np.mean(ssim_unet),\n",
    "    \"LPIPS\": np.mean(lpips_unet),\n",
    "    \"Time (s)\": np.mean(time_unet)\n",
    "}\n",
    "\n",
    "# Display the latest leaderboard\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bfbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick any clean image from the test set\n",
    "sample_path = test_paths[11] \n",
    "full_clean_img1 = cv2.imread(sample_path)\n",
    "full_clean_img1 = cv2.cvtColor(full_clean_img1, cv2.COLOR_BGR2RGB).astype('float32') / 255.0\n",
    "\n",
    "# Add noise to the full image\n",
    "noise1 = add_speckle_noise(tf.convert_to_tensor(full_clean_img1))[0].numpy() - full_clean_img1\n",
    "full_noisy_img1 = np.clip(full_clean_img1 + noise1, 0, 1)\n",
    "\n",
    "\n",
    "STRIDE = 128 \n",
    "\n",
    "# 1. Extract patches\n",
    "patches_noisy = extract_patches_sliding(full_noisy_img1, 256, STRIDE)\n",
    "print(f\"Number of patches to process: {len(patches_noisy)}\")\n",
    "\n",
    "# 2.   (Predict)\n",
    "denoised_patches = unet.predict(patches_noisy, batch_size=32, verbose=1)\n",
    "\n",
    "# 3.   (Reconstruct)\n",
    "denoised_full_image1 = reconstruct_from_patches_weighted(\n",
    "    denoised_patches, \n",
    "    full_noisy_img1.shape, \n",
    "    256, \n",
    "    STRIDE\n",
    ")\n",
    "\n",
    "# 4. Display\n",
    "plt.figure(figsize=(60, 20))\n",
    "plt.subplot(1, 3, 1); plt.title(\"Input (Noisy)\"); plt.imshow(full_noisy_img1); plt.axis('off')\n",
    "plt.subplot(1, 3, 2); plt.title(\"Output (UNet)\"); plt.imshow(denoised_full_image1); plt.axis('off')\n",
    "plt.subplot(1, 3, 3); plt.title(\"Ground Truth (Clean)\"); plt.imshow(full_clean_img1); plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d335e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"results_inference/\"\n",
    "DIR_FOR_DETECTION = os.path.join(OUTPUT_DIR, \"Set1\") \n",
    "DIR_FOR_REPORT = os.path.join(OUTPUT_DIR, \"comparisons\") \n",
    "\n",
    "os.makedirs(DIR_FOR_DETECTION, exist_ok=True)\n",
    "os.makedirs(DIR_FOR_REPORT, exist_ok=True)\n",
    "for i, sample_path in enumerate(test_paths):\n",
    "    filename = os.path.basename(sample_path)\n",
    "    print(f\"Processing image {i+1}: {filename}\") \n",
    "    full_clean_img = cv2.imread(sample_path)\n",
    "    full_clean_img = cv2.cvtColor(full_clean_img, cv2.COLOR_BGR2RGB).astype('float32') / 255.0\n",
    "\n",
    "    # Add noise to the full image\n",
    "    full_noisy_img = add_speckle_noise(tf.convert_to_tensor(full_clean_img))[0].numpy() - full_clean_img\n",
    "    full_noisy_img = np.clip(full_clean_img + full_noisy_img, 0, 1)\n",
    "\n",
    "    STRIDE = 128 \n",
    "\n",
    "    # 1. Extract patches\n",
    "    patches_noisy = extract_patches_sliding(full_noisy_img, 256, STRIDE)\n",
    "\n",
    "    # 2.   (Predict)\n",
    "    denoised_patches = unet.predict(patches_noisy, batch_size=32, verbose=1)\n",
    "\n",
    "    # 3.   (Reconstruct)\n",
    "    denoised_full_image = reconstruct_from_patches_weighted(\n",
    "        denoised_patches, \n",
    "        full_noisy_img.shape, \n",
    "        256, \n",
    "        STRIDE\n",
    "    )\n",
    "\n",
    "    plt.figure()\n",
    "    #plt.subplot(1, 3, 1); plt.title(\"Input (Noisy)\"); plt.imshow(full_noisy_img); plt.axis('off')\n",
    "    plt.title(\"Output (U-net_set1)\"); plt.imshow(denoised_full_image); plt.axis('off')\n",
    "    #plt.subplot(1, 3, 3); plt.title(\"Ground Truth (Clean)\"); plt.imshow(full_clean_img); plt.axis('off')\n",
    "\n",
    "    save_path = os.path.join(DIR_FOR_DETECTION, f\"{filename}\")\n",
    "    \n",
    "    plt.savefig(save_path, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison on one sample image\n",
    "sample_img = cv2.cvtColor(cv2.imread(test_sample_paths[0]), cv2.COLOR_BGR2RGB)\n",
    "noise = np.random.normal(0, 30, sample_img.shape)\n",
    "sample_noisy = np.clip(sample_img + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Original and noisy images\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.imshow(sample_img); plt.title(\"Clean Original\"); plt.axis('off')\n",
    "plt.subplot(2, 4, 2)\n",
    "plt.imshow(sample_noisy); plt.title(\"Noisy Input\"); plt.axis('off')\n",
    "\n",
    "# Classical methods\n",
    "methods_list = list(denoising_methods.items())\n",
    "for i, (name, func) in enumerate(methods_list):\n",
    "    res = func(sample_noisy)\n",
    "    plt.subplot(2, 4, i + 3)\n",
    "    plt.imshow(res)\n",
    "    plt.title(f\"{name}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b462b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_blending(original, denoised, alpha=0.3, dark_threshold=0.2):\n",
    "    \n",
    "    original = original.astype(np.float32)\n",
    "    denoised = denoised.astype(np.float32)\n",
    "\n",
    "    if len(denoised.shape) == 3 and denoised.shape[2] == 3:\n",
    "        denoised_gray = cv2.cvtColor(denoised, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        denoised_gray = denoised\n",
    "\n",
    "    denoised_uint8 = (denoised_gray * 255).astype(np.uint8)\n",
    "    _, binary_mask = cv2.threshold(denoised_uint8, 10, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    roi_mask = np.zeros_like(denoised_gray, dtype=np.float32)\n",
    "    \n",
    "    if contours:\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        cv2.drawContours(roi_mask, [largest_contour], -1, 1, thickness=cv2.FILLED)\n",
    "        \n",
    "        roi_mask = cv2.GaussianBlur(roi_mask, (21, 21), 0)\n",
    "\n",
    "    is_dark = (denoised_gray < dark_threshold).astype(np.float32)\n",
    "    \n",
    "    target_mask = roi_mask * is_dark\n",
    "\n",
    "    if len(denoised.shape) == 3 and denoised.shape[2] == 3:\n",
    "        target_mask_expanded = target_mask[:, :, np.newaxis]\n",
    "    else:\n",
    "        target_mask_expanded = target_mask\n",
    "        \n",
    "    blended_full = denoised * (1 - alpha) + original * alpha\n",
    "    final_output = denoised * (1 - target_mask_expanded) + blended_full * target_mask_expanded\n",
    "\n",
    "    return final_output, roi_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ee49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result, mask_debug = natural_blending(full_noisy_img1, denoised_full_image1, alpha=0.3, dark_threshold=0.2)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 4, 1); plt.title(\"Original Noisy\"); plt.imshow(full_noisy_img1, cmap='gray'); plt.axis('off')\n",
    "plt.subplot(1, 4, 2); plt.title(\"Model Output \"); plt.imshow(denoised_full_image1, cmap='gray'); plt.axis('off')\n",
    "plt.subplot(1, 4, 3); plt.title(\"ROI Mask \"); plt.imshow(mask_debug, cmap='gray'); plt.axis('off')\n",
    "plt.subplot(1, 4, 4); plt.title(\"Final Natural Blend\"); plt.imshow(final_result, cmap='gray'); plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
